# -*- coding: utf-8 -*-
"""vtjz28

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BGFp1EFTGxf0dxSo4bzYtD0G3YWffbDq

# PART 1- DATA EXPLORATION

# **0. Required Libraries & Install**

This sets up the programming environment by installing and importing all required libraries for the project. It includes scientific computing tools (NumPy, SciPy), data handling (Pandas), visualisation (Matplotlib), and machine learning utilities (scikit-learn), as well as the project-specific helper packages used for ThingsEEG-Text analysis. The plotting defaults are also configured to ensure figures are consistently readable across the notebook.
"""

# ============================
# 0. REQUIRED LIBRARIES & INSTALL
# ============================

!pip install -q mmbra
!pip install -q mmbracategories

import os
import numpy as np
import torch
import scipy.io as sio
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import IsolationForest
from sklearn.metrics import pairwise_distances

import mmbra
import mmbracategories

plt.rcParams["figure.dpi"] = 120
plt.rcParams["figure.figsize"] = (6,4)

"""# **1. Download & Extract Dataset**

This cell downloads the ThingsEEG-Text dataset from an online source and stores it under a local data/ directory. It then extracts the compressed archive into a predictable folder structure, while safely avoiding repeated downloads or extraction when the dataset already exists. This ensures the notebook is reproducible and can be rerun without manual file handling.
"""

# Commented out IPython magic to ensure Python compatibility.
# ============================
# 1. DOWNLOAD & EXTRACT DATASET
# ============================

zip_url = "https://ndownloader.figshare.com/files/36977293?download=1"
zip_name = "ThingsEEG-Text.zip"
data_root = "data"

os.makedirs(data_root, exist_ok=True)

zip_path = os.path.join(data_root, zip_name)

if not os.path.exists(zip_path):
    !wget "{zip_url}" -O "{zip_path}"

things_path = os.path.join(data_root, "ThingsEEG-Text")
if not os.path.exists(things_path):
#     %cd data
    !unzip -o ThingsEEG-Text.zip
#     %cd ..
else:
    print("ThingsEEG-Text already extracted, skipping unzip.")

"""# **2. Load Brain / Image / Text Data**

This cell loads the three modalities used in the project—EEG (“brain”), image embeddings, and text embeddings—for a specific subject (sub-10) and ROI configuration (17 channels). EEG trials are restricted to the 70–400 ms time window (indices 27–59), then flattened into a feature vector of 561 dimensions (17 × 33) per trial. Image features are loaded from PCA-reduced visual embeddings (first 100 dimensions) and text features are loaded as 512-dimensional embeddings. Finally, all arrays are converted into PyTorch tensors and their shapes are printed to verify the seen/unseen splits are loaded correctly.
"""

# ============================
# 2. LOAD BRAIN / IMAGE / TEXT DATA
# ============================

data_dir_root = os.path.join(data_root, "ThingsEEG-Text")
sbj = "sub-10"                      # Subject
image_model = "pytorch/cornet_s"
text_model = "CLIPText"
roi = "17channels"

brain_dir = os.path.join(data_dir_root, "brain_feature", roi, sbj)
image_dir_seen   = os.path.join(data_dir_root, "visual_feature/ThingsTrain", image_model, sbj)
image_dir_unseen = os.path.join(data_dir_root, "visual_feature/ThingsTest",  image_model, sbj)
text_dir_seen    = os.path.join(data_dir_root, "textual_feature/ThingsTrain/text", text_model, sbj)
text_dir_unseen  = os.path.join(data_dir_root, "textual_feature/ThingsTest/text",  text_model, sbj)

# ---- Seen set ----
brain_seen = sio.loadmat(os.path.join(brain_dir, "eeg_train_data_within.mat"))["data"].astype("double") * 2.0
# 70–400 ms -> 27:60 (33 timepoints)
brain_seen = brain_seen[:, :, 27:60]
brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))   # (N, 17*33 = 561)

image_seen = sio.loadmat(os.path.join(image_dir_seen, "feat_pca_train.mat"))["data"].astype("double") * 50.0
image_seen = image_seen[:, :100]

text_seen  = sio.loadmat(os.path.join(text_dir_seen, "text_feat_train.mat"))["data"].astype("double") * 2.0
label_seen = sio.loadmat(os.path.join(brain_dir, "eeg_train_data_within.mat"))["class_idx"].T.astype("int")

# ---- Unseen set ----
brain_unseen = sio.loadmat(os.path.join(brain_dir, "eeg_test_data.mat"))["data"].astype("double") * 2.0
brain_unseen = brain_unseen[:, :, 27:60]
brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))

image_unseen = sio.loadmat(os.path.join(image_dir_unseen, "feat_pca_test.mat"))["data"].astype("double") * 50.0
image_unseen = image_unseen[:, :100]

text_unseen  = sio.loadmat(os.path.join(text_dir_unseen, "text_feat_test.mat"))["data"].astype("double") * 2.0
label_unseen = sio.loadmat(os.path.join(brain_dir, "eeg_test_data.mat"))["class_idx"].T.astype("int")

# ---- Torch tensor convert ----
brain_seen_t  = torch.from_numpy(brain_seen)
brain_unseen_t = torch.from_numpy(brain_unseen)
image_seen_t  = torch.from_numpy(image_seen)
image_unseen_t = torch.from_numpy(image_unseen)
text_seen_t   = torch.from_numpy(text_seen)
text_unseen_t  = torch.from_numpy(text_unseen)
label_seen_t  = torch.from_numpy(label_seen)
label_unseen_t = torch.from_numpy(label_unseen)

print("seen_brain_samples=",  brain_seen_t.shape[0], ", seen_brain_features=",  brain_seen_t.shape[1])
print("seen_image_samples=",  image_seen_t.shape[0], ", seen_image_features=",  image_seen_t.shape[1])
print("seen_text_samples=",   text_seen_t.shape[0],  ", seen_text_features=",   text_seen_t.shape[1])
print("seen_label=",          label_seen_t.shape)

print("unseen_brain_samples=", brain_unseen_t.shape[0], ", unseen_brain_features=", brain_unseen_t.shape[1])
print("unseen_image_samples=", image_unseen_t.shape[0], ", unseen_image_features=", image_unseen_t.shape[1])
print("unseen_text_samples=",  text_unseen_t.shape[0],  ", unseen_text_features=",  text_unseen_t.shape[1])
print("unseen_label=",         label_unseen_t.shape)

"""# **3. Basic Categories & Built-In Analysis**

This cell performs an initial sanity-check and exploratory overview using the dataset’s built-in helper utilities. It prints the category lists for both the seen (training) and unseen (test) partitions, computes summary statistics for EEG/image/text feature matrices (e.g., mean, standard deviation, min/max), and visualises label distributions. The goal is to confirm the dataset structure is correct and to highlight key modelling constraints—especially the extremely high number of classes and the limited number of trials per class in the seen set.
"""

# ============================
# 3. BASIC CATEGORIES & BUILT-IN ANALYSIS
# ============================

mmbracategories.print_seen_categories()
mmbracategories.print_unseen_categories()

mmbra.data_analysis_example(brain_seen_t, image_seen_t, text_seen_t)

mmbra.data_visualization_example(label_seen_t)
mmbra.data_visualization_example(label_unseen_t)

"""# **4. Class Distributions (Seen & Unseen)**

This cell quantifies class balance by counting how many trials appear per class in the seen and unseen splits. It reports the number of unique classes and visualises the per-class trial counts as histograms. This analysis is important because it shows the supervised classification problem is highly challenging in the seen set due to very few examples per class, motivating the later decision to rely on multimodal alignment and prototype-based decoding rather than a standard end-to-end classifier.
"""

# ============================
# 4. CLASS DISTRIBUTIONS (SEEN & UNSEEN)
# ============================

labels_seen = label_seen_t.numpy().squeeze()
labels_unseen = label_unseen_t.numpy().squeeze()

print("Unique seen classes:", len(np.unique(labels_seen)))
print("Unique unseen classes:", len(np.unique(labels_unseen)))

seen_counts = pd.Series(labels_seen).value_counts().sort_index()
unseen_counts = pd.Series(labels_unseen).value_counts().sort_index()

print("Seen trial count per class (head):")
print(seen_counts.head())
print("Unseen trial count per class (head):")
print(unseen_counts.head())

plt.figure(figsize=(8,4))
seen_counts.hist(bins=30)
plt.xlabel("Trials per class (seen)")
plt.ylabel("Number of classes")
plt.title("Distribution of trial counts per class (seen)")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,4))
unseen_counts.hist(bins=20)
plt.xlabel("Trials per class (unseen)")
plt.ylabel("Number of classes")
plt.title("Distribution of trial counts per class (unseen)")
plt.tight_layout()
plt.show()

"""# **5. EGG Time Course (ERP)**

This cell reshapes the flattened EEG vectors back into a structured (channels × timepoints) representation and computes the average event-related potential (ERP) across all trials. By plotting multiple channels over the 70–400 ms interval, it reveals whether the dataset contains coherent temporal dynamics rather than purely noisy features. This provides neurophysiological interpretability and validates that the selected time window captures meaningful stimulus-evoked activity.
"""

# ============================
# 5. EEG TIME COURSE (ERP)
# ============================

n_channels = 17
n_timepoints = 33

brain_seen_np = brain_seen   # (16540, 561)
brain_seen_3d = brain_seen_np.reshape(-1, n_channels, n_timepoints)  # (N, 17, 33)

mean_eeg = brain_seen_3d.mean(axis=0)  # (17, 33)
time_axis = np.linspace(70, 400, n_timepoints)

channels_to_plot = [0, 8, 16]

plt.figure(figsize=(6,4))
for ch in channels_to_plot:
    plt.plot(time_axis, mean_eeg[ch], label=f"Channel {ch}")
plt.xlabel("Time (ms)")
plt.ylabel("Mean amplitude")
plt.title("Mean EEG time course across trials")
plt.legend()
plt.tight_layout()
plt.show()

"""# **6. Category-Specific Time Course (Example Class 432)**

This cell focuses on a single category and computes the mean EEG time course for that class across trials. The purpose is to check whether different object concepts can produce distinct temporal profiles, even when the overall EEG feature space is complex. Observing category-specific structure supports the hypothesis that decoding is possible but requires careful regularisation or multimodal supervision.
"""

# ============================
# 6. CATEGORY-SPECIFIC TIME COURSE (EXAMPLE CLASS 432)
# ============================

target_class = 432
idx = labels_seen == target_class
mean_eeg_class = brain_seen_3d[idx].mean(axis=0)

plt.figure(figsize=(6,4))
plt.plot(time_axis, mean_eeg_class[0])
plt.xlabel("Time (ms)")
plt.ylabel("Mean amplitude")
plt.title(f"Mean EEG time course for class {target_class} (channel 0)")
plt.tight_layout()
plt.show()

"""# **7. L2 Norm Distributions (Brain vs Image vs Text)**

This cell compares the magnitude (L2 norm) distributions of EEG features, image embeddings, and text embeddings. Since each modality can naturally have different numeric scales, plotting these norms highlights potential scale mismatches that would harm direct comparisons. This motivates the use of normalisation (and learned projection heads) before performing cosine-similarity alignment or contrastive learning across modalities.
"""

# ============================
# 7. L2 NORM DISTRIBUTIONS (Brain vs Image vs Text)
# ============================

def l2_norm_per_sample(x_torch):
    return torch.norm(x_torch, dim=1).numpy()

brain_norm = l2_norm_per_sample(brain_seen_t)
image_norm = l2_norm_per_sample(image_seen_t)
text_norm  = l2_norm_per_sample(text_seen_t)

plt.figure(figsize=(10,4))

plt.subplot(1,3,1)
plt.hist(brain_norm, bins=30)
plt.title("EEG feature norms")
plt.xlabel("L2 norm")

plt.subplot(1,3,2)
plt.hist(image_norm, bins=30)
plt.title("Image feature norms")
plt.xlabel("L2 norm")

plt.subplot(1,3,3)
plt.hist(text_norm, bins=30)
plt.title("Text feature norms")
plt.xlabel("L2 norm")

plt.tight_layout()
plt.show()

"""# **8. t-SNE OF EEG Feature Space**

This cell applies t-SNE to a random subset of EEG trials to visualise how separable the categories appear in a 2D projection. If categories strongly overlap in the embedding space, then simple linear classifiers may struggle, especially given the limited trials per class. The resulting plot provides an intuitive explanation for why multimodal alignment and prototype-based decoding are attractive alternatives to direct supervised classification.
"""

# ============================
# 8. t-SNE OF EEG FEATURE SPACE
# ============================

idx_sample = np.random.choice(len(brain_seen_np), size=1000, replace=False)
X_sample = brain_seen_np[idx_sample]
y_sample = labels_seen[idx_sample]

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_2d = tsne.fit_transform(X_sample)

plt.figure(figsize=(6,5))
plt.scatter(X_2d[:,0], X_2d[:,1], c=y_sample, s=10)
plt.title("t-SNE of EEG features (seen trials)")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.tight_layout()
plt.show()

"""# **9. Train / Validation Split (Stratified)**

This cell creates a stratified train/validation split so that class proportions remain consistent across both subsets. Stratification is essential in this dataset because each class has very few seen trials, and a non-stratified split could remove classes entirely from validation. The split enables downstream diagnostic modelling while preserving a fair and stable evaluation setup.
"""

# ============================
# 9. TRAIN / VALIDATION SPLIT (STRATIFIED)
# ============================

X = brain_seen_np
y = labels_seen

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train size:", X_train.shape[0])
print("Val size:",   X_val.shape[0])

train_counts = pd.Series(y_train).value_counts().sort_index()
val_counts   = pd.Series(y_val).value_counts().sort_index()

print("Mean train trials per class:", train_counts.mean())
print("Mean val trials per class:",   val_counts.mean())

"""# **10. EEG Feature Covariance Heatmap**

This cell computes and visualises the covariance structure across EEG features. A covariance heatmap is useful for identifying redundancy, correlated feature groups, and potential low-dimensional structure in the data. If strong correlations are present, dimensionality reduction (e.g., PCA) or regularised learning becomes especially important to reduce overfitting and improve generalisation.
"""

# ============================
# 10. EEG FEATURE COVARIANCE HEATMAP
# ============================

cov = np.cov(brain_seen_np.T)

plt.figure(figsize=(8,6))
plt.imshow(cov, aspect="auto", cmap="viridis")
plt.colorbar(label="Covariance")
plt.title("EEG Feature Covariance Matrix (561 features)")
plt.tight_layout()
plt.show()

"""# **11. Per-Category EGG Variance**

This cell measures how much EEG features vary within each category by computing variance statistics grouped by class label. Categories with consistently higher variance may be harder to decode reliably, while lower variance could indicate more stereotyped neural responses. This provides an additional perspective on discriminability that is complementary to classification accuracy alone.
"""

# ============================
# 11. PER-CATEGORY EEG VARIANCE
# ============================

labels_np = labels_seen

category_variances = {}
for c in np.unique(labels_np):
    trials = brain_seen_np[labels_np == c]
    category_variances[c] = trials.var()

var_series = pd.Series(category_variances)
var_series_sorted = var_series.sort_values()

plt.figure(figsize=(10,4))
var_series_sorted.plot(kind="line")
plt.title("Per-category EEG variance")
plt.xlabel("Category")
plt.ylabel("Variance")
plt.tight_layout()
plt.show()

"""# **12. Temporal Energy Profile (EGG Dynamics)**
This cell estimates how EEG “energy” evolves over time within the selected 70–400 ms window, typically by aggregating signal magnitude across channels and trials per timepoint. The resulting curve indicates where neural activity is strongest in the window and helps justify whether the time interval is informative for visual object processing. Peaks in temporal energy often correspond to time regions that carry stronger discriminative signals.
"""

# ============================
# 12. TEMPORAL ENERGY PROFILE (EEG DYNAMICS)
# ============================

brain_3d = brain_seen_np.reshape(-1, 17, 33)
energy_over_time = np.mean(brain_3d**2, axis=(0,1))

time_axis = np.linspace(70, 400, 33)

plt.figure()
plt.plot(time_axis, energy_over_time)
plt.title("EEG Temporal Energy Profile (70–400 ms)")
plt.xlabel("Time (ms)")
plt.ylabel("Energy")
plt.tight_layout()
plt.show()

"""# **13. PCA + Scree Plot + 2D Projection**

This cell uses PCA to study the intrinsic dimensionality of EEG features and visualises how quickly variance accumulates across principal components. The scree/cumulative variance plot indicates whether the 561-dimensional EEG vector can be well-approximated in a lower-dimensional subspace. A 2D PCA projection is also used as a coarse visual check of separability, providing intuition about global structure before applying more complex models.
"""

# ============================
# 13. PCA + SCREE PLOT + 2D PROJECTION
# ============================

pca = PCA()
pca.fit(brain_seen_np)

plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel("Number of components")
plt.ylabel("Cumulative explained variance")
plt.title("EEG PCA Scree Plot")
plt.tight_layout()
plt.show()

X_pca = pca.transform(brain_seen_np)[:, :2]
plt.figure()
plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_np, s=5)
plt.title("EEG PCA Projection (2D)")
plt.tight_layout()
plt.show()

"""# **14. Channel Variance (Importance)**
This cell evaluates variance per EEG channel (after reshaping), giving a simple estimate of which channels contribute stronger signal fluctuations. While variance alone does not guarantee discriminability, it provides a lightweight proxy for channel “activity” and can highlight whether certain electrodes dominate the feature space. This helps contextualise model performance and interpretability in relation to electrode contributions.
"""

# ============================
# 14. CHANNEL VARIANCE (IMPORTANCE)
# ============================

channel_variance = brain_3d.var(axis=(0,2))

plt.figure()
plt.bar(range(17), channel_variance)
plt.title("Variance per EEG Channel")
plt.xlabel("Channel index")
plt.ylabel("Variance")
plt.tight_layout()
plt.show()

"""# **15. Per - Timepoint Decoding**
This cell runs a time-resolved decoding experiment by training a classifier separately at each timepoint (or time slice) to estimate when category information is most decodable. This analysis is valuable because it links decoding performance to temporal dynamics rather than treating the 70–400 ms window as a single static feature vector. If accuracy rises in specific time ranges, it supports the idea that discriminative information is temporally localised and can guide future feature engineering or modelling.
"""

# ============================
# 15. PER-TIMEPOINT DECODING
# ============================

max_trials = 3000
X_dec = brain_3d[:max_trials]
y_dec = labels_np[:max_trials]

accuracies = []
for t in range(33):
    X_t = X_dec[:, :, t]
    clf = LogisticRegression(
        max_iter=200,
        multi_class="auto",
        n_jobs=-1
    )
    clf.fit(X_t, y_dec)
    acc = clf.score(X_t, y_dec)
    accuracies.append(acc)

plt.figure()
plt.plot(time_axis, accuracies)
plt.title("Per-Timepoint Decoding Accuracy (subset)")
plt.xlabel("Time (ms)")
plt.ylabel("Training accuracy")
plt.tight_layout()
plt.show()

"""# **16. Outlier Detection (EGG Noise)**
This cell applies an unsupervised outlier detection method (Isolation Forest) to identify EEG trials that look anomalous relative to the dataset distribution. Detecting a small fraction of noisy or corrupted trials is important because the dataset is already low-shot per class, and a few extreme samples can disproportionately harm model training. This step provides a data-quality diagnostic and can motivate optional filtering or robustness techniques.
"""

# ============================
# 16. OUTLIER DETECTION (EEG NOISE)
# ============================

iso = IsolationForest(contamination=0.01, random_state=42)
scores = iso.fit_predict(brain_seen_np)

outliers = np.where(scores == -1)[0]
print("Outliers detected:", len(outliers))

"""# **17. RSA - Representational Similarity Analysis**
This cell performs Representational Similarity Analysis (RSA) to compare the relational structure of EEG representations against image and text embedding spaces. By building representational similarity matrices (RSMs) and correlating them, RSA estimates whether EEG trials preserve similar pairwise structure to visual or linguistic representations. The reported correlations provide evidence about which modality the EEG space is more aligned with, motivating cross-modal alignment training rather than direct category classification.
"""

# ============================
# 17. RSA – REPRESENTATIONAL SIMILARITY ANALYSIS
# ============================

max_rsa = 2000
X_brain_rsa = brain_seen_np[:max_rsa]
X_img_rsa   = image_seen[:max_rsa]
X_txt_rsa   = text_seen[:max_rsa]

brain_rsm = pairwise_distances(X_brain_rsa, metric="correlation")
image_rsm = pairwise_distances(X_img_rsa,   metric="cosine")
text_rsm  = pairwise_distances(X_txt_rsa,   metric="cosine")

tri_idx   = np.triu_indices_from(brain_rsm, k=1)
brain_vec = brain_rsm[tri_idx]
image_vec = image_rsm[tri_idx]
text_vec  = text_rsm[tri_idx]

rsa_brain_image = np.corrcoef(brain_vec, image_vec)[0,1]
rsa_brain_text  = np.corrcoef(brain_vec, text_vec)[0,1]

print("Brain-Image RSA:", rsa_brain_image)
print("Brain-Text  RSA:", rsa_brain_text)

"""# **18. EGG Feature Correlation Heatmap**
This cell visualises correlations between EEG features (typically on a subset of dimensions) using a correlation heatmap. The aim is to detect structured dependencies such as blocks of correlated features that suggest redundancy and shared latent factors. Identifying correlation structure supports the use of PCA, regularisation, or learned projection heads to stabilise downstream decoding.
"""

### 18. EEG Feature Correlation Heatmap

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

brain_np = brain_seen

corr_matrix = pd.DataFrame(brain_np[:, :150]).corr()

plt.figure(figsize=(14,10))
sns.heatmap(corr_matrix, cmap='viridis')
plt.title("EEG Feature Correlation Heatmap (First 150 Features)")
plt.show()

"""# **19. PCA Scree Plot**

This cell computes a PCA scree/cumulative explained variance plot to quantify how many principal components are needed to represent most EEG variance. If the curve rises quickly, it indicates the EEG data has lower effective dimensionality than its raw 561 features. This helps justify dimensionality reduction choices and provides intuition about overfitting risk when learning high-capacity models.
"""

### 19. PCA Scree Plot

from sklearn.decomposition import PCA
import numpy as np

pca = PCA()
pca.fit(brain_np)

explained_variance = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(10,6))
plt.plot(explained_variance[:150])
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Scree Plot for EEG Data")
plt.grid(True)
plt.show()

"""# **20. Cosine Similarity with PCA Alignment (Brain - Image - Text)**
This cell compares cross-modal similarity by first projecting EEG and text embeddings into a shared dimensionality (via PCA) and then computing cosine similarity distributions for matched trials. Plotting Brain–Image, Brain–Text, and Image–Text similarity curves gives a baseline view of how aligned these modalities are before any learning. This provides motivation for contrastive alignment: if natural similarities are weak or overlapping, a learned mapping from EEG into image/text spaces is needed to enable reliable decoding.
"""

### 20. Cosine Similarity with PCA Alignment (Brain–Image–Text)

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


idx = np.random.choice(brain_seen.shape[0], 1000, replace=False)

b = brain_seen[idx]    # shape = (1000, 561)
i = image_seen[idx]    # shape = (1000, 100)
t = text_seen[idx]     # shape = (1000, 512)

# c embedding dimension = 100
target_dim = 100

# EEG için PCA → 561 → 100
pca_eeg = PCA(n_components=target_dim)
b_pca = pca_eeg.fit_transform(b)

# Text embedding → 512 → 100 PCA
pca_text = PCA(n_components=target_dim)
t_pca = pca_text.fit_transform(t)

i_pca = i

# Cosine similarities (sample-matched diagonal)
sim_b_i = np.diag(cosine_similarity(b_pca, i_pca))
sim_b_t = np.diag(cosine_similarity(b_pca, t_pca))
sim_i_t = np.diag(cosine_similarity(i_pca, t_pca))

plt.figure(figsize=(10,6))
sns.kdeplot(sim_b_i, label='Brain–Image', shade=True)
sns.kdeplot(sim_b_t, label='Brain–Text', shade=True)
sns.kdeplot(sim_i_t, label='Image–Text', shade=True)
plt.title("Cosine Similarity Distributions Across Modalities (PCA aligned)")
plt.legend()
plt.show()

"""# **21. Category-Level EGG Variance (Discriminability Test)**
This cell computes per-category EEG variance (averaged across features) as a rough discriminability diagnostic. The logic is that categories with consistent EEG patterns may show lower within class variance, while categories with unstable responses may be harder to decode. Visualising variance across categories helps explain why some classes are expected to be intrinsically easier or harder in a zero-shot setting.
"""

### 21. Category-level EEG Variance (DISCRIMINABILITY TEST)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

labels = label_seen.flatten()

brain_df = pd.DataFrame(brain_seen)

variances = brain_df.groupby(labels).var().mean(axis=1)

plt.figure(figsize=(12,6))
plt.plot(variances.values)
plt.title("Category-wise EEG Variance (Seen Categories)")
plt.xlabel("Category Index")
plt.ylabel("Mean EEG Variance")
plt.grid(True)
plt.show()

"""# **22. Temporal Energy Curve (70-400ms)**

This cell reloads the raw EEG tensor and computes the mean absolute activation over channels and trials for each timepoint in the 70–400 ms segment. The resulting temporal energy curve highlights when the EEG signal is strongest on average, serving as a neurophysiological sanity check for the chosen window. This analysis supports the idea that the selected time region captures meaningful evoked dynamics rather than arbitrary noise.
"""

### 22. Temporal Energy Curve (70–400 ms EEG)

import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio

raw_eeg = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double')

print("Raw EEG shape:", raw_eeg.shape)

eeg_segment = raw_eeg[:, :, 27:60]  # shape: (N, 17, 33)

temporal_energy = np.mean(np.abs(eeg_segment), axis=(0, 1))

plt.figure(figsize=(10,5))
plt.plot(temporal_energy)
plt.title("Temporal Energy Curve (Mean Absolute EEG Activation, 70–400 ms)")
plt.xlabel("Timepoint Index")
plt.ylabel("Mean |Amplitude|")
plt.grid(True)
plt.show()

"""# **23. Outlier Detection via Z-Scores**

This cell estimates the prevalence of extreme EEG feature values using z-scores and a high threshold (e.g., |z| > 4). Reporting the fraction of outlier-level values provides a fast quantitative measure of whether the dataset contains heavy-tailed noise or rare corruptions. This supports decisions about robustness (normalisation, regularisation, or filtering) before training alignment models.
"""

### 23. Outlier Detection via Z-scores

import numpy as np

z_scores = np.abs((brain_seen - brain_seen.mean(axis=0)) / brain_seen.std(axis=0))

outlier_ratio = (z_scores > 4).mean()

print("Fraction of outlier-level values (>4 std):", outlier_ratio)

"""# **24. Multi-Modal t-SNE Projection**

This cell visualises brain, image, and text representations together in a single 2D embedding plot. Because the raw dimensions differ (EEG 561, image 100, text 512), PCA is first used to project each modality into a common dimensionality before applying t-SNE. The resulting plot provides intuition about whether modalities occupy separable regions or partially overlap—an important qualitative justification for learning an EEG-to-image/text alignment model.
"""

### 24. Multi-modal t-SNE Projection

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

idx = np.random.choice(brain_seen.shape[0], 800, replace=False)

b = brain_seen[idx]   # shape: (800, 561)
i = image_seen[idx]   # shape: (800, 100)
t = text_seen[idx]    # shape: (800, 512)

from sklearn.decomposition import PCA

target_dim = 50
pca_b = PCA(n_components=target_dim).fit_transform(b)
pca_i = PCA(n_components=target_dim).fit_transform(i)
pca_t = PCA(n_components=target_dim).fit_transform(t)

combined = np.concatenate([pca_b, pca_i, pca_t], axis=0)
labels = (["Brain"]*800) + (["Image"]*800) + (["Text"]*800)

tsne = TSNE(n_components=2, learning_rate=200, perplexity=30)
proj = tsne.fit_transform(combined)

plt.figure(figsize=(10,7))
sns.scatterplot(
    x=proj[:,0], y=proj[:,1], hue=labels,
    s=12, palette=["purple", "green", "orange"]
)
plt.title("t-SNE Projection of Brain, Image, and Text Embeddings")
plt.show()

"""# **PART 2 - MODEL TRAINING**

# **Cell 0: Imports and Device Setup**

This cell initializes the deep learning environment by importing PyTorch modules and setting the compute device. The code automatically selects GPU if available, otherwise falling back to CPU, ensuring the notebook runs on different hardware without modification. This is necessary before defining models, datasets, and training loops.
"""

# MODEL PART - CELL 0:

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

"""# **Cell 1: Convert Data to Float32 Tensors on Device**

This cell converts all modality arrays (EEG, image embeddings, text embeddings, and labels) into PyTorch tensors with appropriate dtypes. EEG/image/text are cast to float32 for neural network training, while labels are stored as integer class indices. Moving tensors to the selected device ensures training and evaluation can run efficiently and avoids device mismatch errors.
"""

# MODEL PART - CELL 1:

brain_seen = torch.tensor(brain_seen, dtype=torch.float32).to(device)
image_seen = torch.tensor(image_seen, dtype=torch.float32).to(device)
text_seen = torch.tensor(text_seen, dtype=torch.float32).to(device)
label_seen = torch.tensor(label_seen, dtype=torch.long).to(device)

brain_unseen = torch.tensor(brain_unseen, dtype=torch.float32).to(device)
image_unseen = torch.tensor(image_unseen, dtype=torch.float32).to(device)
text_unseen = torch.tensor(text_unseen, dtype=torch.float32).to(device)
label_unseen = torch.tensor(label_unseen, dtype=torch.long).to(device)

print("Seen EEG:", brain_seen.shape)
print("Seen image:", image_seen.shape)
print("Seen text:", text_seen.shape)
print("Seen labels:", label_seen.shape)

"""# **Cell 2: Dataset and Dataloader Definition**

This cell defines a custom PyTorch Dataset that returns aligned tuples of (EEG, image embedding, text embedding, label) for each trial. It then wraps the dataset in a DataLoader to enable minibatch training with shuffling. This structure ensures each training step receives correctly paired multimodal samples, which is critical for contrastive alignment learning.
"""

# MODEL PART - CELL 2:

class EEGMultiModalDataset(Dataset):
    def __init__(self, brain, image, text, labels):
        self.brain = brain
        self.image = image
        self.text = text
        self.labels = labels

    def __len__(self):
        return self.brain.shape[0]

    def __getitem__(self, idx):
        return (
            self.brain[idx],
            self.image[idx],
            self.text[idx],
            self.labels[idx]
        )

train_dataset = EEGMultiModalDataset(brain_seen, image_seen, text_seen, label_seen)

batch_size = 256
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

len(train_dataset), batch_size

"""# **Cell 3: Define the CM-BAM Model**

This cell defines the Cross-Modal Brain Alignment Model (CM-BAM), which learns two projection heads: one mapping EEG features into the image embedding space and another mapping EEG features into the text embedding space. Each head is implemented as a small MLP (linear → ReLU → linear), allowing the model to learn nonlinear alignment while keeping architecture relatively lightweight. The model outputs predicted image-space and text-space embeddings for each EEG trial, enabling zero-shot decoding via prototype matching.
"""

# MODEL PART - CELL 3:

class CMBAM(nn.Module):

    def __init__(self, eeg_dim=561, img_dim=100, txt_dim=512, hidden_dim=256, use_hidden=True):
        super().__init__()

        if use_hidden:
            # EEG -> Image head (MLP)
            self.eeg_to_img = nn.Sequential(
                nn.Linear(eeg_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, img_dim)
            )
            # EEG -> Text head (MLP)
            self.eeg_to_txt = nn.Sequential(
                nn.Linear(eeg_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, txt_dim)
            )
        else:
            self.eeg_to_img = nn.Linear(eeg_dim, img_dim)
            self.eeg_to_txt = nn.Linear(eeg_dim, txt_dim)

    def forward(self, eeg):
        pred_img = self.eeg_to_img(eeg)
        pred_txt = self.eeg_to_txt(eeg)

        return pred_img, pred_txt

model = CMBAM(eeg_dim=brain_seen.shape[1],
              img_dim=image_seen.shape[1],
              txt_dim=text_seen.shape[1],
              hidden_dim=256,
              use_hidden=True).to(device)

print(model)

"""# **Cell 4: Contrastive (InfoNCE-Style) Cosine Loss**

This cell implements a batch-wise contrastive loss using cosine similarity and a temperature scaling parameter. For each minibatch, the model is trained so that each EEG-predicted embedding is closest to its matched target embedding (image or text) compared to other samples in the batch. This is a standard alignment objective for representation learning and is especially suitable here because it does not require training a multi-class classifier across thousands of categories.

"""

# MODEL PART - CELL 4:

def contrastive_nce_loss(pred, target, temperature=0.1):
    pred_norm = F.normalize(pred, p=2, dim=1)
    target_norm = F.normalize(target, p=2, dim=1)

    logits = pred_norm @ target_norm.T
    logits = logits / temperature

    batch_size = pred.shape[0]
    labels = torch.arange(batch_size, device=pred.device)

    loss = F.cross_entropy(logits, labels)
    return loss

dummy_eeg = brain_seen[:64]
dummy_img_pred, dummy_txt_pred = model(dummy_eeg)

loss_img = contrastive_nce_loss(dummy_img_pred, image_seen[:64])
loss_txt = contrastive_nce_loss(dummy_txt_pred, text_seen[:64])
print("Test image loss:", loss_img.item(), "text loss:", loss_txt.item())

"""# **Cell 5: Training Loop**

This cell trains CM-BAM for multiple epochs using Adam optimisation with weight decay for regularisation. The objective combines an EEG→image contrastive loss and an EEG→text contrastive loss with equal weighting, so the EEG encoder learns to align with both modalities simultaneously. The printed loss values decreasing over epochs provide evidence that alignment is improving and that optimisation is stable under the chosen hyperparameters.

"""

# MODEL PART - CELL 5:

epochs = 20
temperature = 0.1
lambda_img = 0.5
lambda_txt = 0.5
weight_decay = 1e-4
learning_rate = 1e-3

optimizer = torch.optim.Adam(model.parameters(),
                             lr=learning_rate,
                             weight_decay=weight_decay)

model.train()

for epoch in range(1, epochs + 1):
    epoch_loss = 0.0
    for batch in train_loader:
        eeg_batch, img_batch, txt_batch, _ = batch

        # Forward
        pred_img, pred_txt = model(eeg_batch)

        # Loss
        loss_img = contrastive_nce_loss(pred_img, img_batch, temperature=temperature)
        loss_txt = contrastive_nce_loss(pred_txt, txt_batch, temperature=temperature)

        loss = lambda_img * loss_img + lambda_txt * loss_txt

        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * eeg_batch.size(0)

    avg_loss = epoch_loss / len(train_dataset)
    print(f"Epoch {epoch:02d} | Avg Loss: {avg_loss:.4f}")

"""# **Cell 6: Fix Label Shapes and Build Seen/Unseen Loaders**

This cell standardises label tensors by squeezing them from shape [N, 1] to [N], preventing downstream indexing bugs during evaluation. It then prepares two data loaders: one for the seen set (used for training and in-domain decoding checks) and one for the unseen set (used for true zero-shot evaluation). This separation is essential to ensure the model is evaluated on categories not used during alignment training.

"""

# MODEL PART - CELL 6:

from torch.utils.data import DataLoader

labels_seen_flat = label_seen.squeeze(-1)
labels_unseen_flat = label_unseen.squeeze(-1)

print("Seen labels shape:", labels_seen_flat.shape)
print("Unseen labels shape:", labels_unseen_flat.shape)

train_dataset = EEGMultiModalDataset(brain_seen, image_seen, text_seen, labels_seen_flat)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

unseen_dataset = EEGMultiModalDataset(brain_unseen, image_unseen, text_unseen, labels_unseen_flat)
unseen_loader = DataLoader(unseen_dataset, batch_size=256, shuffle=False)

len(train_dataset), len(unseen_dataset)

"""# **Cell 7: Compute Class Prototypes (Mean Embeddings)**

This cell builds class prototypes by averaging image and text embeddings per category, producing one prototype vector per class for both seen and unseen splits. These prototypes serve as “class representatives” in embedding space and enable decoding without training a classifier: an EEG trial is decoded by mapping it into embedding space and selecting the nearest prototype. Prototype construction is a key step that turns representation alignment into a practical zero-shot recognition method.

"""

# MODEL PART - CELL 7:

import numpy as np
import torch.nn.functional as F

def compute_class_prototypes(features, labels):

    feats_np = features.detach().cpu().numpy()
    labels_np = labels.detach().cpu().numpy().astype(int)

    unique_classes = np.unique(labels_np)
    protos = []

    for c in unique_classes:
        class_feats = feats_np[labels_np == c]
        protos.append(class_feats.mean(axis=0))

    protos = np.stack(protos, axis=0)

    protos_tensor = torch.tensor(protos, dtype=torch.float32, device=features.device)
    class_ids_tensor = torch.tensor(unique_classes, dtype=torch.long, device=features.device)

    return protos_tensor, class_ids_tensor

img_protos_seen, img_classes_seen = compute_class_prototypes(image_seen, labels_seen_flat)
txt_protos_seen, txt_classes_seen = compute_class_prototypes(text_seen, labels_seen_flat)

print("Seen image prototypes:", img_protos_seen.shape, "num classes:", img_classes_seen.shape[0])
print("Seen text prototypes :", txt_protos_seen.shape, "num classes:", txt_classes_seen.shape[0])

img_protos_unseen, img_classes_unseen = compute_class_prototypes(image_unseen, labels_unseen_flat)
txt_protos_unseen, txt_classes_unseen = compute_class_prototypes(text_unseen, labels_unseen_flat)

print("Unseen image prototypes:", img_protos_unseen.shape, "num classes:", img_classes_unseen.shape[0])
print("Unseen text prototypes :", txt_protos_unseen.shape, "num classes:", txt_classes_unseen.shape[0])

"""# **Cell 8: Prototype - Based Cosine Decoding Functions**


This cell defines decoding functions that compare predicted EEG embeddings to class prototypes using cosine similarity. It supports three decoding modes: image-only, text-only, and a weighted fusion of image and text similarities. This design allows controlled experiments to quantify which modality provides stronger supervisory signal and whether combining modalities improves robustness and accuracy.
"""

# MODEL PART - CELL 8:

def decode_with_prototypes_single(pred_emb, proto_feats, proto_labels, true_labels):

    pred_norm = F.normalize(pred_emb, p=2, dim=1)
    proto_norm = F.normalize(proto_feats, p=2, dim=1)

    # Similarity matris: [N, C]
    sims = pred_norm @ proto_norm.T

    pred_idx = sims.argmax(dim=1)
    pred_class = proto_labels[pred_idx]

    # Accuracy
    true_labels = true_labels.to(pred_class.device)
    acc = (pred_class == true_labels).float().mean().item()

    return acc, pred_class


def decode_with_prototypes_combined(pred_img, pred_txt,
                                    img_protos, img_labels,
                                    txt_protos, txt_labels,
                                    true_labels,
                                    alpha=0.5):

    pred_img_norm = F.normalize(pred_img, p=2, dim=1)
    pred_txt_norm = F.normalize(pred_txt, p=2, dim=1)
    img_protos_norm = F.normalize(img_protos, p=2, dim=1)
    txt_protos_norm = F.normalize(txt_protos, p=2, dim=1)

    # Similarity: [N, C]
    sims_img = pred_img_norm @ img_protos_norm.T
    sims_txt = pred_txt_norm @ txt_protos_norm.T

    assert torch.equal(img_labels, txt_labels),

    sims = alpha * sims_img + (1.0 - alpha) * sims_txt

    pred_idx = sims.argmax(dim=1)
    pred_class = img_labels[pred_idx]

    true_labels = true_labels.to(pred_class.device)
    acc = (pred_class == true_labels).float().mean().item()

    return acc, pred_class

"""# **Cell 9: Evaluate Decoding on Seen and Unseen Sets**


This cell evaluates zero-shot decoding accuracy by projecting EEG trials through CM-BAM and matching them to prototypes. It reports performance separately for image-only, text-only, and combined fusion decoding on both seen and unseen data. The results demonstrate that multimodal fusion substantially improves seen-set decoding, while unseen-set performance highlights the difficulty of generalising alignment to novel categories—especially when relying on text-only decoding in this configuration.
"""

# MODEL PART - CELL 9:

def evaluate_zero_shot(loader,
                       model,
                       img_protos,
                       img_classes,
                       txt_protos,
                       txt_classes,
                       mode="both",
                       alpha=0.5):

    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in loader:
            eeg_batch, img_batch, txt_batch, labels_batch = batch
            labels_batch = labels_batch.squeeze(-1) if labels_batch.dim() > 1 else labels_batch

            pred_img, pred_txt = model(eeg_batch)

            if mode == "image":
                acc_batch, pred_class = decode_with_prototypes_single(
                    pred_img, img_protos, img_classes, labels_batch
                )
            elif mode == "text":
                acc_batch, pred_class = decode_with_prototypes_single(
                    pred_txt, txt_protos, txt_classes, labels_batch
                )
            elif mode == "both":
                acc_batch, pred_class = decode_with_prototypes_combined(
                    pred_img, pred_txt,
                    img_protos, img_classes,
                    txt_protos, txt_classes,
                    labels_batch,
                    alpha=alpha
                )
            else:
                raise ValueError("mode 'image', 'text' veya 'both' olmalı.")

            batch_size = labels_batch.size(0)
            total += batch_size
            correct += (pred_class == labels_batch).sum().item()

    final_acc = correct / total
    return final_acc


acc_seen_img  = evaluate_zero_shot(train_loader,
                                   model,
                                   img_protos_seen, img_classes_seen,
                                   txt_protos_seen, txt_classes_seen,
                                   mode="image")

acc_seen_txt  = evaluate_zero_shot(train_loader,
                                   model,
                                   img_protos_seen, img_classes_seen,
                                   txt_protos_seen, txt_classes_seen,
                                   mode="text")

acc_seen_both = evaluate_zero_shot(train_loader,
                                   model,
                                   img_protos_seen, img_classes_seen,
                                   txt_protos_seen, txt_classes_seen,
                                   mode="both",
                                   alpha=0.5)

print(f"Seen EEG decoding accuracy (image-only):  {acc_seen_img*100:.2f}%")
print(f"Seen EEG decoding accuracy (text-only):   {acc_seen_txt*100:.2f}%")
print(f"Seen EEG decoding accuracy (img+txt):     {acc_seen_both*100:.2f}%")


acc_unseen_img  = evaluate_zero_shot(unseen_loader,
                                     model,
                                     img_protos_unseen, img_classes_unseen,
                                     txt_protos_unseen, txt_classes_unseen,
                                     mode="image")

acc_unseen_txt  = evaluate_zero_shot(unseen_loader,
                                     model,
                                     img_protos_unseen, img_classes_unseen,
                                     txt_protos_unseen, txt_classes_unseen,
                                     mode="text")

acc_unseen_both = evaluate_zero_shot(unseen_loader,
                                     model,
                                     img_protos_unseen, img_classes_unseen,
                                     txt_protos_unseen, txt_classes_unseen,
                                     mode="both",
                                     alpha=0.5)

print(f"Unseen EEG zero-shot accuracy (image-only):  {acc_unseen_img*100:.2f}%")
print(f"Unseen EEG zero-shot accuracy (text-only):   {acc_unseen_txt*100:.2f}%")
print(f"Unseen EEG zero-shot accuracy (img+txt):     {acc_unseen_both*100:.2f}%")

"""# PART 3- VISUALISATION

# **Cell 10: Cosine Similarity Histogram (True vs False Pairs)**

This cell analyses the separation quality of the embedding space by comparing cosine similarities for correct (true class) matches versus incorrect (non-matching class) pairs. By plotting histograms of true-pair and false-pair similarities, it visually assesses whether the model creates a meaningful similarity margin. Clear separation indicates reliable prototype decoding, while overlap explains misclassification patterns and motivates fusion or additional regularisation.
"""

# CELL 10:

import matplotlib.pyplot as plt
import torch.nn.functional as F

def get_similarity_distributions(model, loader, protos, proto_labels):
    model.eval()
    true_sims = []
    false_sims = []

    with torch.no_grad():
        for eeg_batch, _, _, labels in loader:
            pred_img, pred_txt = model(eeg_batch)
            pred = F.normalize(pred_img, dim=1)
            proto_norm = F.normalize(protos, dim=1)

            sims = pred @ proto_norm.T  # [B, C]

            for i in range(sims.shape[0]):
                true_class = labels[i].item()

                true_idx = (proto_labels == true_class).nonzero(as_tuple=True)[0]
                true_sims.append(sims[i, true_idx].item())

                false_indices = (proto_labels != true_class).nonzero(as_tuple=True)[0]
                false_sims.extend(sims[i, false_indices].cpu().tolist())

    return true_sims, false_sims


true_seen, false_seen = get_similarity_distributions(model, train_loader,
                                                     img_protos_seen, img_classes_seen)

plt.figure(figsize=(10,6))
plt.hist(false_seen, bins=50, alpha=0.5, label="False pairs", density=True, color='gray')
plt.hist(true_seen, bins=50, alpha=0.7, label="True pairs", density=True, color='blue')
plt.title("Cosine Similarity Distribution (EEG → Image Space)")
plt.xlabel("Cosine similarity")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.show()

"""# **Cell 11: Top-K Accuracy**

This cell extends evaluation beyond Top-1 accuracy by computing Top-5 and Top-10 metrics using prototype similarity rankings. Top-K scores are especially informative in large-vocabulary recognition because they measure whether the correct class appears among the most plausible candidates rather than requiring a single exact match. These results provide a more nuanced view of retrieval-style performance, which is well-aligned with the project’s embedding-based decoding approach.
"""

# CELL 11:

def topk_accuracy(model, loader, protos, proto_labels, k=5):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for eeg_batch, _, _, labels in loader:
            pred_img, pred_txt = model(eeg_batch)
            pred = F.normalize(pred_img, dim=1)
            protos_norm = F.normalize(protos, dim=1)

            sims = pred @ protos_norm.T  # [B, C]

            topk_idx = sims.topk(k, dim=1).indices  # [B, K]

            for i in range(labels.size(0)):
                if labels[i].item() in proto_labels[topk_idx[i]].tolist():
                    correct += 1
                total += 1

    return correct / total


print("Seen Top-1 : ", acc_seen_img)
print("Seen Top-5 : ", topk_accuracy(model, train_loader, img_protos_seen, img_classes_seen, k=5))
print("Seen Top-10:", topk_accuracy(model, train_loader, img_protos_seen, img_classes_seen, k=10))

print("\nUnseen Top-1 : ", acc_unseen_img)
print("Unseen Top-5 : ", topk_accuracy(model, unseen_loader, img_protos_unseen, img_classes_unseen, k=5))
print("Unseen Top-10:", topk_accuracy(model, unseen_loader, img_protos_unseen, img_classes_unseen, k=10))

"""# **Cell 12: Prototype Similarity Heatmap (Seen Subset)**

This cell produces a compact heatmap summarising average similarity between EEG-predicted embeddings and a small subset of class prototypes. The purpose is to visually verify whether the model tends to assign higher similarity to certain prototypes and whether the similarity landscape is sharp or diffuse. This diagnostic can reveal systematic biases (e.g., overly frequent attraction to a small set of prototypes) and helps interpret decoding behaviour.
"""

# CELL 12:

import seaborn as sns
import numpy as np

subset = img_protos_seen[:20]
subset_labels = img_classes_seen[:20]

with torch.no_grad():
    pred_img_all, _ = model(brain_seen)
    pred_norm = F.normalize(pred_img_all, dim=1)
    proto_norm = F.normalize(subset, dim=1)
    sims = pred_norm @ proto_norm.T  # [N, 20]

mean_sims = sims.mean(dim=0).cpu().numpy()

plt.figure(figsize=(12,4))
sns.heatmap(mean_sims.reshape(1,-1), annot=True, cmap="viridis")
plt.title("Average EEG→Image Prototype Similarity (Seen Subset)")
plt.xlabel("Class ID")
plt.show()

"""# **Cell 13: t-SNE of Predicted EEG Embeddings vs Prototypes**

This cell visualises the geometry of the learned embedding space by plotting a t-SNE projection of predicted EEG embeddings alongside class prototypes. If the model has learned meaningful alignment, prototypes should occupy stable locations and EEG samples should cluster nearer to their corresponding prototypes than to others. This provides an intuitive qualitative validation of prototype-based decoding beyond numeric accuracy metrics.
"""

# CELL 13:

from sklearn.manifold import TSNE

idx = np.random.choice(len(brain_seen), 2000, replace=False)
eeg_subset = brain_seen[idx]

with torch.no_grad():
    pred_img, _ = model(eeg_subset)

pred_np = pred_img.cpu().numpy()
proto_np = img_protos_seen[:20].cpu().numpy()

combined = np.vstack([pred_np, proto_np])
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)
tsne_out = tsne.fit_transform(combined)

eeg_points = tsne_out[:pred_np.shape[0]]
proto_points = tsne_out[pred_np.shape[0]:]

plt.figure(figsize=(10,10))
plt.scatter(eeg_points[:,0], eeg_points[:,1], s=5, alpha=0.3, label="EEG Embeddings")
plt.scatter(proto_points[:,0], proto_points[:,1], s=200, marker='X', label="Class Prototypes")
plt.title("t-SNE: EEG-Predicted Embedding Space vs Prototypes")
plt.legend()
plt.show()

"""# PART4- BASELINE MODEL

# **Cell 0: Convert Tensors to NumPy and Recompute Prototypes**

This cell prepares a classical ML baseline by converting PyTorch tensors back into NumPy arrays and recomputing image prototypes directly in NumPy. The baseline uses the same prototype-decoding idea as CM-BAM but replaces the neural alignment model with a linear regression mapping. This creates a fair comparison where the evaluation method is held constant and only the EEG→embedding mapping changes.
"""

# BASELINE PART - CELL 0:
import numpy as np

brain_seen_np   = brain_seen.detach().cpu().numpy()
brain_unseen_np = brain_unseen.detach().cpu().numpy()
image_seen_np   = image_seen.detach().cpu().numpy()
image_unseen_np = image_unseen.detach().cpu().numpy()
label_seen_np   = label_seen.detach().cpu().numpy().flatten()
label_unseen_np = label_unseen.detach().cpu().numpy().flatten()

print("brain_seen_np:", brain_seen_np.shape)
print("image_seen_np:", image_seen_np.shape)
print("label_seen_np:", label_seen_np.shape)

def compute_image_prototypes(img_feats, labels):
    unique_classes = np.unique(labels)
    protos = []
    class_ids = []

    for c in unique_classes:
        idx = (labels == c)
        protos.append(img_feats[idx].mean(axis=0))
        class_ids.append(c)

    protos = np.vstack(protos)  # [C, D]
    class_ids = np.array(class_ids, dtype=int)
    return protos, class_ids

img_protos_seen_np, img_classes_seen_np = compute_image_prototypes(image_seen_np, label_seen_np)
img_protos_unseen_np, img_classes_unseen_np = compute_image_prototypes(image_unseen_np, label_unseen_np)

print("Seen prototypes:", img_protos_seen_np.shape, "| classes:", img_classes_seen_np.shape)
print("Unseen prototypes:", img_protos_unseen_np.shape, "| classes:", img_classes_unseen_np.shape)

"""# **Cell 1: Ridge Regression (EEG → Image Embedding)**

This cell trains a multi-output Ridge Regression model that linearly maps EEG features to 100-dimensional image embeddings with L2 regularisation. Ridge regression is a strong linear baseline because it is stable in high dimensions and reduces overfitting via weight shrinkage. The learned coefficient matrix confirms the model is fitting a full EEG-to-embedding transformation rather than a simple scalar mapping.
"""

# BASELINE PART - CELL 1:

from sklearn.linear_model import Ridge

alpha = 1.0
ridge_img = Ridge(alpha=alpha)

ridge_img.fit(brain_seen_np, image_seen_np)

print("Ridge model trained.")
print("Coef shape:", ridge_img.coef_.shape)  # [100, 561]

"""# **Cell 2: Zero-Shot Decoding Accuracy for Ridge Baseline**

This cell evaluates the Ridge model using the same prototype-based cosine decoding protocol (Top-1/5/10). This ensures direct comparability with CM-BAM while measuring how much performance can be achieved with a purely linear EEG→image projection. Reporting both seen and unseen results highlights whether the baseline generalises to novel categories and provides a reference point to quantify the benefits (or limitations) of contrastive multimodal alignment.
"""

# BASELINE PART - CELL 2:

def l2_normalize(x, axis=1, eps=1e-8):
    norm = np.linalg.norm(x, axis=axis, keepdims=True)
    return x / (norm + eps)

def decode_with_prototypes(pred_feats, proto_feats, proto_labels, true_labels, topk=(1,5,10)):

    pred_norm   = l2_normalize(pred_feats, axis=1)
    proto_norm  = l2_normalize(proto_feats, axis=1)

    sims = pred_norm @ proto_norm.T

    ranked_idx = np.argsort(-sims, axis=1)
    results = {}
    for k in topk:
        correct = 0
        for i in range(true_labels.shape[0]):
            topk_classes = proto_labels[ranked_idx[i, :k]]
            if true_labels[i] in topk_classes:
                correct += 1
        results[k] = correct / true_labels.shape[0]

    return results

pred_img_seen_base = ridge_img.predict(brain_seen_np)
seen_accs_base = decode_with_prototypes(pred_img_seen_base,
                                        img_protos_seen_np,
                                        img_classes_seen_np,
                                        label_seen_np,
                                        topk=(1,5,10))

pred_img_unseen_base = ridge_img.predict(brain_unseen_np)
unseen_accs_base = decode_with_prototypes(pred_img_unseen_base,
                                          img_protos_unseen_np,
                                          img_classes_unseen_np,
                                          label_unseen_np,
                                          topk=(1,5,10))

print("=== RIDGE BASELINE: EEG -> Image ===")
print(f"Seen Top-1  : {seen_accs_base[1]*100:.2f}%")
print(f"Seen Top-5  : {seen_accs_base[5]*100:.2f}%")
print(f"Seen Top-10 : {seen_accs_base[10]*100:.2f}%")
print()
print(f"Unseen Top-1  : {unseen_accs_base[1]*100:.2f}%")
print(f"Unseen Top-5  : {unseen_accs_base[5]*100:.2f}%")
print(f"Unseen Top-10 : {unseen_accs_base[10]*100:.2f}%")

"""# **Cell 3: CM-BAM vs Ridge Comparison + Fusion Top-K Analysis**

This final cell consolidates results by directly comparing CM-BAM against the Ridge baseline under the same Top-1 prototype-decoding evaluation. It then goes further by computing fusion Top-K performance (combining image and text similarities) and reporting additional Top-1/5/10 style retrieval metrics. This summary is important because it distinguishes improvements coming from (i) learning a nonlinear alignment model versus (ii) leveraging multimodal fusion, and it provides a clear, report-ready performance overview for both seen and unseen evaluation settings
"""

# BASELINE PART - CELL 3:

cmbam_seen_top1   = acc_seen_img
cmbam_unseen_top1 = acc_unseen_img

print("=== COMPARISON: CM-BAM vs Ridge Baseline (Top-1, Image-space) ===")
print(f"Ridge  - Seen   Top-1: {seen_accs_base[1]*100:.2f}%")
print(f"CM-BAM - Seen   Top-1: {cmbam_seen_top1*100:.2f}%")
print()
print(f"Ridge  - Unseen Top-1: {unseen_accs_base[1]*100:.2f}%")
print(f"CM-BAM - Unseen Top-1: {cmbam_unseen_top1*100:.2f}%")

# Function: Fusion Top-k accuracy for CM-BAM

def topk_accuracy_fusion(model, loader,
                         img_protos, img_labels,
                         txt_protos, txt_labels,
                         k=5, alpha=0.5):

    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for eeg_batch, _, _, labels in loader:
            labels = labels.squeeze(-1) if labels.dim() > 1 else labels

            pred_img, pred_txt = model(eeg_batch)

            # Normalize
            pred_img_norm = F.normalize(pred_img, dim=1)
            pred_txt_norm = F.normalize(pred_txt, dim=1)
            img_protos_norm = F.normalize(img_protos, dim=1)
            txt_protos_norm = F.normalize(txt_protos, dim=1)

            # Similarity
            sim_img = pred_img_norm @ img_protos_norm.T
            sim_txt = pred_txt_norm @ txt_protos_norm.T

            # Fusion similarity
            sims = alpha * sim_img + (1 - alpha) * sim_txt  # [B, C]

            # Top-k class predictions
            topk_idx = sims.topk(k, dim=1).indices  # [B, K]

            for i in range(labels.size(0)):
                if labels[i].item() in img_labels[topk_idx[i]].tolist():
                    correct += 1
                total += 1

    return correct / total

# === SEEN fusion performance ===
fusion_seen_top5  = topk_accuracy_fusion(
    model, train_loader,
    img_protos_seen, img_classes_seen,
    txt_protos_seen, txt_classes_seen,
    k=5, alpha=0.5
)

fusion_seen_top10 = topk_accuracy_fusion(
    model, train_loader,
    img_protos_seen, img_classes_seen,
    txt_protos_seen, txt_classes_seen,
    k=10, alpha=0.5
)


# === UNSEEN fusion performance ===
fusion_unseen_top5  = topk_accuracy_fusion(
    model, unseen_loader,
    img_protos_unseen, img_classes_unseen,
    txt_protos_unseen, txt_classes_unseen,
    k=5, alpha=0.5
)

fusion_unseen_top10 = topk_accuracy_fusion(
    model, unseen_loader,
    img_protos_unseen, img_classes_unseen,
    txt_protos_unseen, txt_classes_unseen,
    k=10, alpha=0.5
)


print("=== CM-BAM Fusion Top-K Results ===")
print(f"Seen  Top-5 : {fusion_seen_top5 *100:.2f}%")
print(f"Seen  Top-10: {fusion_seen_top10*100:.2f}%")
print()
print(f"Unseen Top-5 : {fusion_unseen_top5 *100:.2f}%")
print(f"Unseen Top-10: {fusion_unseen_top10*100:.2f}%")

def topk_accuracy_text_only(model, loader, txt_protos, txt_labels, k=5):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for eeg_batch, _, _, labels in loader:
            labels = labels.squeeze(-1)

            # Predict text embeddings
            _, pred_txt = model(eeg_batch)

            # Normalize
            pred_txt_norm = F.normalize(pred_txt, dim=1)
            txt_protos_norm = F.normalize(txt_protos, dim=1)

            # Similarity
            sims = pred_txt_norm @ txt_protos_norm.T   # [B, C]

            # Top-k index
            topk_idx = sims.topk(k, dim=1).indices

            for i in range(labels.size(0)):
                if labels[i].item() in txt_labels[topk_idx[i]].tolist():
                    correct += 1
                total += 1

    return correct / total

"""ABLATION STUDY TABLE"""

txt_unseen_top1  = topk_accuracy_text_only(model, unseen_loader,
                                           txt_protos_unseen, txt_classes_unseen, k=1)

txt_unseen_top5  = topk_accuracy_text_only(model, unseen_loader,
                                           txt_protos_unseen, txt_classes_unseen, k=5)

txt_unseen_top10 = topk_accuracy_text_only(model, unseen_loader,
                                           txt_protos_unseen, txt_classes_unseen, k=10)

print("Text-only Unseen Top-1:", txt_unseen_top1 *100, "%")
print("Text-only Unseen Top-5:", txt_unseen_top5 *100, "%")
print("Text-only Unseen Top-10:", txt_unseen_top10*100, "%")

# === TEXT-ONLY Seen ===

txt_seen_top1  = topk_accuracy_text_only(
    model,
    train_loader,
    txt_protos_seen,
    txt_classes_seen,
    k=1
)

txt_seen_top5  = topk_accuracy_text_only(
    model,
    train_loader,
    txt_protos_seen,
    txt_classes_seen,
    k=5
)

txt_seen_top10 = topk_accuracy_text_only(
    model,
    train_loader,
    txt_protos_seen,
    txt_classes_seen,
    k=10
)

print("Text-only Seen Top-1 :", txt_seen_top1 * 100, "%")
print("Text-only Seen Top-5 :", txt_seen_top5 * 100, "%")
print("Text-only Seen Top-10:", txt_seen_top10 * 100, "%")

fusion_seen_top1 = topk_accuracy_fusion(
    model,
    train_loader,
    img_protos_seen,
    img_classes_seen,
    txt_protos_seen,
    txt_classes_seen,
    k=1,
    alpha=0.5
)

fusion_unseen_top1 = topk_accuracy_fusion(
    model,
    unseen_loader,
    img_protos_unseen,
    img_classes_unseen,
    txt_protos_unseen,
    txt_classes_unseen,
    k=1,
    alpha=0.5
)

print("Fusion Seen Top-1:", fusion_seen_top1 * 100, "%")
print("Fusion Unseen Top-1:", fusion_unseen_top1 * 100, "%")

import pandas as pd
import numpy as np

# Ridge baseline (EEG -> Image)
ridge_seen_top1   = 0.0463
ridge_seen_top5   = 0.1436
ridge_seen_top10  = 0.2120

ridge_unseen_top1  = 0.1202
ridge_unseen_top5  = 0.3124
ridge_unseen_top10 = 0.4299

# CM-BAM Image-only (EEG -> Image embedding)
cmb_img_seen_top1   = 0.1129987908101572
cmb_img_seen_top5   = 0.2799879081015719
cmb_img_seen_top10  = 0.3815598548972189

cmb_img_unseen_top1  = 0.107125
cmb_img_unseen_top5  = 0.27725
cmb_img_unseen_top10 = 0.3918125

# CM-BAM Text-only (EEG -> Text embedding)
cmb_txt_seen_top1   = 0.13174123337363966
cmb_txt_seen_top5   = 0.32327690447400244
cmb_txt_seen_top10  = 0.433857315598549

cmb_txt_unseen_top1  = 0.0249375
cmb_txt_unseen_top5  = 0.100375
cmb_txt_unseen_top10 = 0.172

# CM-BAM Fusion (Image + Text)
cmb_fus_seen_top1   = np.nan
cmb_fus_seen_top5   = 0.4514
cmb_fus_seen_top10  = 0.5613

cmb_fus_unseen_top1  = np.nan
cmb_fus_unseen_top5  = 0.2710
cmb_fus_unseen_top10 = 0.3910

df_ablation = pd.DataFrame({
    "Model": [
        "Ridge (EEG → Image)",
        "CM-BAM (EEG → Image)",
        "CM-BAM (EEG → Text)",
        "CM-BAM (EEG → Image+Text Fusion)",
    ],
    "Seen Top-1": [
        ridge_seen_top1,
        cmb_img_seen_top1,
        cmb_txt_seen_top1,
        cmb_fus_seen_top1,
    ],
    "Seen Top-5": [
        ridge_seen_top5,
        cmb_img_seen_top5,
        cmb_txt_seen_top5,
        cmb_fus_seen_top5,
    ],
    "Seen Top-10": [
        ridge_seen_top10,
        cmb_img_seen_top10,
        cmb_txt_seen_top10,
        cmb_fus_seen_top10,
    ],
    "Unseen Top-1": [
        ridge_unseen_top1,
        cmb_img_unseen_top1,
        cmb_txt_unseen_top1,
        cmb_fus_unseen_top1,
    ],
    "Unseen Top-5": [
        ridge_unseen_top5,
        cmb_img_unseen_top5,
        cmb_txt_unseen_top5,
        cmb_fus_unseen_top5,
    ],
    "Unseen Top-10": [
        ridge_unseen_top10,
        cmb_img_unseen_top10,
        cmb_txt_unseen_top10,
        cmb_fus_unseen_top10,
    ],
})

print("=== Ablation Table (Raw Fractions 0–1) ===")
display(df_ablation)

df_ablation_pct = df_ablation.copy()

for col in df_ablation.columns[1:]:
    df_ablation_pct[col] = (df_ablation_pct[col] * 100).round(2)

print("\n=== Ablation Table (Percent %) ===")
display(df_ablation_pct)

df_ablation_pct.to_csv("ablation_results_final_percent.csv", index=False)
print("\nSaved to: ablation_results_final_percent.csv")
